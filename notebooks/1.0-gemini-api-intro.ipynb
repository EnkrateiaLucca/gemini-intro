{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you read this???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make predictions or decisions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini for Image Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the bounding box detections:\n",
      "```json\n",
      "[\n",
      "  {\"box_2d\": [338, 58, 383, 175], \"label\": \"arrow\"}\n",
      "]\n",
      "```\n",
      "The image shows the architecture and capabilities of the Gemini model. It highlights that Gemini models build on top of Transformer decoders and are trained to handle interleaved textual, audio, and visual inputs. The visual encoding is inspired by previous work, and the models can natively output images using discrete image tokens. The figure illustrates how the model supports various input types (text, audio, image, video) and produces both text and image outputs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "my_file = client.files.upload(file=\"./assets-resources/sample-image.png\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[my_file, \"Caption this image.\"],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To pass image data inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The diagram in the picture is Figure 2, which illustrates how Gemini models support interleaved sequences of text, image, audio, and video as inputs and can output responses with interleaved image and text. It shows how different input modalities (text, audio, image, video) are processed through a Transformer and then decoded into either image or text output.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "with open('./assets-resources/sample-image.png', 'rb') as f:\n",
    "    image_bytes = f.read()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "model='gemini-2.0-flash',\n",
    "contents=[\n",
    "    types.Part.from_bytes(\n",
    "    data=image_bytes,\n",
    "    mime_type='image/png',\n",
    "    ),\n",
    "    'What is the diagram in this picture?'\n",
    "]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more examples for working with images in Gemini [here](https://ai.google.dev/gemini-api/docs/image-understanding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summary of the document in bullet points, covering the key aspects:\n",
      "\n",
      "**Overall Theme:**\n",
      "\n",
      "*   The document describes AlphaFold, a new deep learning approach for protein structure prediction that significantly improves accuracy compared to previous methods.\n",
      "\n",
      "**Key Findings & Results:**\n",
      "\n",
      "*   AlphaFold uses a neural network to predict distances between pairs of amino acid residues, which convey more structural information than contact predictions alone.\n",
      "*   The network constructs a protein-specific potential of mean force, which can be optimized via gradient descent to generate accurate 3D structures.\n",
      "*   In the CASP13 competition, AlphaFold achieved high-accuracy structures (TM-scores >= 0.7) for 24 out of 43 free modeling domains, surpassing the next best method.\n",
      "*   Distance predictions correlate well with true distances, with uncertainty in those predictions also captured by the network.\n",
      "*   The distogram accuracy predicts the realized structure's accuracy.\n",
      "\n",
      "**Methods:**\n",
      "\n",
      "*   Employs a convolutional neural network trained on PDB structures to predict inter-residue distances (dij).\n",
      "*   Uses a differentiable protein geometry model based on backbone torsion angles (φ, ψ).\n",
      "*   Optimizes structure through gradient descent, repeating optimizations from sampled initializations.\n",
      "*   Utilizes techniques like MSA analysis, data augmentation, auxiliary losses, and cropping in training.\n",
      "*   Incorporates factors to prevent steric clashes (Rosetta's Vscore2_smooth).\n",
      "\n",
      "**Significance & Applications:**\n",
      "\n",
      "*   AlphaFold can predict previously unknown folds to high accuracy.\n",
      "*   Its increased accuracy may lead to insights into protein function and malfunction, especially when homologous proteins are unavailable.\n",
      "*   The method offers improved interface prediction for protein-protein interactions.\n",
      "*   Enhances binding pocket prediction.\n",
      "*   Improves molecular replacement in crystallography.\n",
      "\n",
      "**Network Details:**\n",
      "\n",
      "*   The network architecture is a deep two-dimensional dilated convolutional residual network.\n",
      "*   Features multiple layers including batchnorm, projection, convolution, and ELU nonlinearities.\n",
      "*   Trained with stochastic gradient descent using cross-entropy loss.\n",
      "\n",
      "**Attribution Analysis:**\n",
      "\n",
      "*   The researchers applied Integrated Gradients to understand which input features most affect the network's predictions.\n",
      "\n",
      "*The paper provides extended descriptions of:*\n",
      "\n",
      "*   Data sources and their preparation.\n",
      "*   Detailed experimental setup.\n",
      "*   Model parameters.\n",
      "*   Data augmentation strategies.\n",
      "*   Structure prediction, and validation metrics.\n",
      "\n",
      "**Availability:**\n",
      "\n",
      "*   Source code, weights, and CASP13 input data will be made available.\n",
      "* The authors have competing financial interests in that they filed multiple provisional patent applications regarding the work.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import httpx\n",
    "\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "doc_url = \"https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf\"\n",
    "\n",
    "# Retrieve and encode the PDF byte\n",
    "doc_data = httpx.get(doc_url).content\n",
    "\n",
    "prompt = \"Summarize this document in bullet points\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.0-flash\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=doc_data,\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For locally stored pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here is a 3-page markdown report summarizing the practical tips and relevant information from the Google Prompt Engineering guide.\n",
      "\n",
      "---\n",
      "\n",
      "# Prompt Engineering Guide: Practical Summary (Page 1/3)\n",
      "\n",
      "## 1. Introduction to Prompt Engineering\n",
      "\n",
      "*   **Core Idea:** Prompt engineering is the iterative process of designing effective inputs (prompts) to guide Large Language Models (LLMs) toward desired outputs. It's essential because LLMs are prediction engines, and the prompt sets the context for that prediction.\n",
      "*   **Accessibility:** You don't need to be a data scientist; anyone can write prompts, but crafting *effective* ones takes practice and iteration.\n",
      "*   **Goal:** To create prompts that are clear, specific, and provide sufficient context, leading to accurate, relevant, and useful LLM responses. Inadequate prompts cause ambiguity and poor results.\n",
      "*   **Scope:** This guide focuses on prompting models like Gemini directly (via API or tools like Vertex AI Studio) where configuration is accessible.\n",
      "\n",
      "## 2. Essential LLM Output Configuration\n",
      "\n",
      "*Before* focusing solely on the prompt text, configure the model's output parameters. These significantly impact the results:\n",
      "\n",
      "*   **Output Length (Max Tokens):**\n",
      "    *   Sets the maximum number of tokens the model will generate.\n",
      "    *   **Practical Tip:** Be mindful of costs, latency, and energy use (more tokens = higher). Don't rely on this alone for succinctness; adjust the prompt too. Crucial for techniques like ReAct to prevent excessive output. Too short can truncate output (e.g., invalid JSON).\n",
      "*   **Sampling Controls (Temperature, Top-K, Top-P):** These control the randomness and creativity of the output.\n",
      "    *   **Temperature:**\n",
      "        *   Controls randomness. Lower values (~0.1-0.3) = more deterministic, focused, factual. Higher values (~0.7-1.0) = more creative, diverse, potentially unexpected.\n",
      "        *   **Practical Tip:** Use `0` for tasks with a single correct answer (math, strict data extraction). Start around `0.2` for factual but slightly flexible tasks, and `0.7-0.9` for creative tasks. Be wary of very high temps causing incoherence or the \"repetition loop bug\".\n",
      "    *   **Top-K:**\n",
      "        *   Considers only the `K` most likely next tokens. Lower `K` = more restricted/conservative. Higher `K` = more diverse. `K=1` is deterministic (like Temp 0).\n",
      "        *   **Practical Tip:** Start around `30-40`. Lower `K` (~20) for more factual, higher `K` (~40+) for creative.\n",
      "    *   **Top-P (Nucleus Sampling):**\n",
      "        *   Considers the smallest set of tokens whose cumulative probability exceeds `P`. Lower `P` = more conservative. Higher `P` (~0.95-1.0) = more diverse. `P=0` (or very small) often defaults to the single most likely token. `P=1` considers all tokens.\n",
      "        *   **Practical Tip:** Often used *instead* of or *with* Top-K. A common starting point is `0.95`. Lower `P` (~0.9) for factual, higher `P` (~0.99) for creative.\n",
      "    *   **Putting it Together:** The model typically filters by Top-K and Top-P first, then applies Temperature to the remaining candidates. Extreme settings in one can make others irrelevant (e.g., Temp 0 ignores K/P; K=1 ignores Temp/P).\n",
      "    *   **Starting Point Recommendation:** Temp `0.2`, Top-P `0.95`, Top-K `30` for balanced results. Adjust based on desired creativity/factuality.\n",
      "\n",
      "## 3. Foundational Prompting Techniques\n",
      "\n",
      "*   **Zero-Shot Prompting:**\n",
      "    *   Provide only the task description or question without any examples.\n",
      "    *   `Example: Classify the following movie review: [Review Text]`\n",
      "    *   **Practical Tip:** Simplest method, good starting point. May fail for complex tasks or when specific output formats are needed.\n",
      "*   **One-Shot / Few-Shot Prompting:**\n",
      "    *   Provide one (one-shot) or multiple (few-shot) examples of the task and desired output.\n",
      "    *   `Example (Few-Shot Sentiment):`\n",
      "        `Review: \"Loved it!\" Sentiment: Positive`\n",
      "        `Review: \"Boring.\" Sentiment: Negative`\n",
      "        `Review: \"It was okay.\" Sentiment: Neutral`\n",
      "        `Review: \"[New Review Text]\" Sentiment:`\n",
      "    *   **Practical Tip:** Highly effective for guiding the model on structure, style, and task logic. Use 3-5 high-quality, diverse examples as a rule of thumb. Include edge cases if needed. Ensure examples are accurate, as errors confuse the model.\n",
      "\n",
      "---\n",
      "\n",
      "# Prompt Engineering Guide: Practical Summary (Page 2/3)\n",
      "\n",
      "## 4. Intermediate Prompting Techniques\n",
      "\n",
      "*   **System, Contextual, and Role Prompting:**\n",
      "    *   **System Prompt:** Defines the overall task, fundamental purpose, or constraints (e.g., \"Translate the following text to French.\", \"Only return JSON.\").\n",
      "    *   **Contextual Prompt:** Provides specific background information relevant to the *current* task or query (e.g., \"Given the previous conversation about user preferences, suggest a suitable product.\").\n",
      "    *   **Role Prompt:** Assigns a persona or identity to the LLM (e.g., \"Act as a pirate.\", \"You are a helpful travel guide specialized in budget travel.\").\n",
      "    *   **Practical Tip:** Use **Role Prompting** to control tone, style, and expertise (e.g., \"Explain this concept like I'm five.\", \"Write in a formal, academic style.\"). Combine these types as needed (e.g., a Role prompt can include Context).\n",
      "*   **Step-Back Prompting:**\n",
      "    *   Ask the LLM a more general, abstract question related to the specific task *first*. Then, use the answer to that general question as context when asking the specific task prompt.\n",
      "    *   **Practical Tip:** Improves reasoning by activating broader knowledge. Useful for complex problems or mitigating bias. Requires two LLM calls.\n",
      "*   **Chain of Thought (CoT) Prompting:**\n",
      "    *   Instruct the LLM to break down its reasoning process step-by-step before giving the final answer. Simply add phrases like \"Let's think step by step.\"\n",
      "    *   `Example: Q: [Math Problem]. Let's think step by step. A:`\n",
      "    *   **Practical Tip:** Significantly improves performance on tasks requiring reasoning (math, logic puzzles). Provides interpretability. Works well combined with few-shot examples showing the reasoning steps. Use **Temperature 0** for CoT tasks. Ensure the final answer comes *after* the reasoning steps. More tokens = higher cost/latency.\n",
      "*   **Self-Consistency:**\n",
      "    *   An enhancement to CoT. Run the same CoT prompt multiple times with a higher temperature (to generate diverse reasoning paths). Select the most frequent final answer (majority vote).\n",
      "    *   **Practical Tip:** Improves accuracy over basic CoT, especially for complex reasoning. Significantly increases cost due to multiple runs.\n",
      "*   **Tree of Thoughts (ToT):**\n",
      "    *   (Advanced) Explores multiple reasoning paths simultaneously, forming a tree structure. Better for complex exploration tasks. Less common in basic prompt engineering.\n",
      "*   **ReAct (Reason + Act):**\n",
      "    *   Enables LLMs to use external tools (like search APIs, code interpreters) by interleaving reasoning steps (`Thought:`) with actions (`Action:`, `Action Input:`) and observing results (`Observation:`).\n",
      "    *   **Practical Tip:** Foundational for building agents. Requires external frameworks (e.g., LangChain) and tool setup (API keys). Needs careful management of the prompt history (context) sent back to the LLM in each step. Restrict output length to avoid runaway actions.\n",
      "*   **Automatic Prompt Engineering (APE):**\n",
      "    *   Use an LLM to generate variations of an initial prompt for a specific task. Evaluate these generated prompts (manually or using metrics like BLEU/ROUGE) and select the best one.\n",
      "    *   **Practical Tip:** Can help discover effective prompt phrasing, especially for training data generation. Iterative process.\n",
      "\n",
      "## 5. Code Prompting Specifics\n",
      "\n",
      "*   LLMs like Gemini can understand and generate code.\n",
      "*   **Use Cases:**\n",
      "    *   **Writing Code:** Provide a description of the desired functionality. (e.g., \"Write a Python script to rename files in a folder, prepending 'draft_'\").\n",
      "    *   **Explaining Code:** Paste code and ask for an explanation. (e.g., \"Explain this Bash script line by line.\").\n",
      "    *   **Translating Code:** Provide code in one language and ask for another. (e.g., \"Translate this Bash script to Python.\").\n",
      "    *   **Debugging & Reviewing Code:** Provide code and the error message, ask for debugging help, or ask for general improvements/review.\n",
      "*   **Practical Tips:**\n",
      "    *   **ALWAYS TEST GENERATED CODE.** LLMs can make subtle or significant errors.\n",
      "    *   Be specific about the language, libraries, and desired functionality.\n",
      "    *   For debugging, provide the full error message and relevant code snippet.\n",
      "    *   In tools like Vertex AI Studio, use the 'Markdown' view for code output to preserve formatting (especially Python indentation).\n",
      "\n",
      "---\n",
      "\n",
      "# Prompt Engineering Guide: Practical Summary (Page 3/3)\n",
      "\n",
      "## 6. Best Practices for Effective Prompting\n",
      "\n",
      "*   **Provide Examples (Few-Shot):** (Reiteration) Often the single most effective technique. Show, don't just tell.\n",
      "*   **Design with Simplicity:** Clear, concise language. Avoid jargon or unnecessary info. If it's confusing to you, it's likely confusing to the model.\n",
      "    *   **Tip:** Use clear action verbs (e.g., `Summarize`, `Classify`, `Generate`, `Translate`, `Extract`, `Rewrite`).\n",
      "*   **Be Specific About the Output:** Clearly define the desired format, length, style, content, and target audience. Don't be vague (e.g., \"Write a 3-paragraph blog post for beginners...\" vs. \"Write about consoles.\").\n",
      "*   **Use Instructions over Constraints:** Tell the model *what to do* rather than only *what not to do*. Constraints are okay for safety guardrails or strict formatting but can be less effective or conflicting.\n",
      "    *   `DO: Summarize the text in 3 bullet points.`\n",
      "    *   `LESS EFFECTIVE: Do not write a long summary. Do not use paragraphs.`\n",
      "*   **Control Max Token Length:** Use configuration or specify length in the prompt (e.g., \"...in under 100 words,\" \"...in a single sentence\").\n",
      "*   **Use Variables in Prompts:** Use placeholders (like `{city}` or `$user_input`) to make prompts reusable and dynamic. Essential for integrating prompts into applications.\n",
      "*   **Experiment Iteratively:** Try different phrasing, formats (question vs. instruction), styles, examples, configurations, and even different models/versions. Prompt engineering is not a one-shot process.\n",
      "*   **Mix Classes (Few-Shot Classification):** When providing examples for classification, ensure the examples cover different classes and aren't all clustered together to avoid order bias.\n",
      "*   **Adapt to Model Updates:** Newer model versions may have different capabilities or respond differently. Re-test prompts with new versions.\n",
      "*   **Experiment with Output Formats (JSON/XML):**\n",
      "    *   For non-creative tasks (extraction, classification, structured data), explicitly ask for output in JSON or XML.\n",
      "    *   **Benefits:** Consistent structure, easier parsing in applications, can enforce data types, reduces hallucination likelihood.\n",
      "    *   **Tip:** Provide the desired schema or an example JSON structure in the prompt (few-shot). Be mindful of token limits, as JSON is verbose. Use tools like the `json-repair` library (Python) to fix truncated/malformed JSON output.\n",
      "*   **Working with Schemas (Input):** Provide a JSON Schema definition along with the JSON input data. This helps the LLM understand the structure and focus on relevant fields, especially for complex or large inputs.\n",
      "*   **Collaborate:** If possible, have multiple people attempt prompt design and compare results.\n",
      "*   **DOCUMENT EVERYTHING:**\n",
      "    *   **Crucial:** Keep detailed records of your prompt attempts.\n",
      "    *   **Template Fields:** Prompt Name/Version, Goal, Model Used, Temperature, Top-K, Top-P, Max Tokens, Full Prompt Text, Output(s), Outcome (OK/Not OK/Sometimes OK), Feedback/Notes, Hyperlink (if saved in a tool like Vertex AI Studio).\n",
      "    *   **Why:** Enables learning, debugging, re-testing on new models, and avoids re-doing work.\n",
      "    *   **Tip:** Store prompts in separate files from application code for maintainability. Consider automated testing/evaluation for prompts in production.\n",
      "\n",
      "## 7. Final Takeaway\n",
      "\n",
      "Effective prompt engineering is an iterative cycle: **Craft -> Test -> Analyze -> Document -> Refine.** It requires understanding the LLM's configuration options, leveraging different prompting techniques (especially examples), clearly stating intent, and meticulously documenting experiments to achieve consistent, high-quality results.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import pathlib\n",
    "import httpx\n",
    "\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "# Uncomment this to download from the internet and save it locally\n",
    "# doc_url = \"https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf\"\n",
    "# filepath = pathlib.Path('./assets-resources/prompt-eng-guide-google.pdf')\n",
    "# filepath.write_bytes(httpx.get(doc_url).content)\n",
    "\n",
    "# This example assumes the pdf is stored locally\n",
    "# Retrieve and encode the PDF byte\n",
    "filepath = pathlib.Path('./assets-resources/prompt-eng-guide-google.pdf')\n",
    "\n",
    "prompt = \"Write markdown style 3 page report on this prompt engineering guide with just the practical tips and relevant information.\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.5-pro-preview-03-25\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=filepath.read_bytes(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples with PDFs [here](https://ai.google.dev/gemini-api/docs/document-processing?lang=python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Gemini Docs for All Capabilities\n",
    "\n",
    "- [Audio Understanding](https://ai.google.dev/gemini-api/docs/audio)\n",
    "- [Video Understanding](https://ai.google.dev/gemini-api/docs/video-understanding)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
